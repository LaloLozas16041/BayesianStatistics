# MOTIVACIÓN DEL ENFOQUE BAYESIANO

La Estadística Bayesiana es crucial en Estadística, Ciencia de Datos y Machine Learning. Se aplica en medicina, estimando enfermedades y en marketing digital para evaluar anuncios. Este enfoque se actualiza con datos nuevos, contrario a los métodos "online" estáticos.

Se destaca en pruebas A/B, seleccionando la opción que maximiza beneficios. Por ejemplo, Facebook optimiza qué anuncios mostrar para beneficiar al usuario y al anunciante. La Estadística Bayesiana se basa en la interpretación de la probabilidad como una medida de credibilidad.

Se abordarán los fundamentos, como la definición de probabilidad y el razonamiento dentro del marco bayesiano, preparando para métodos más avanzados de inferencia bayesiana y análisis de datos.

## Conceptos Básicos

### Distribución Conjunta, Marginales y Condicionales

Vamos a suponer que tenemos dos variables aleatorias $A$ y $B$, las distribuciones marginales serían:

- $p(A)$
- $p(B)$

La distribución conjunta la denotaremos como $p(A, B)$. La coma es una forma abreviada de decir "y".

La distribución condicional, a la que nos referimos como $p(A|B)$ o $p(B|A)$.

La barra vertical es el símbolo que usamos para representar una condición. La distribución conjunta es la más general porque es a partir de esta distribución que podemos calcular todo lo demás.

Podemos calcular la distribución marginal si tenemos la distribución conjunta, de la siguiente manera:

- $p(A) = \sum_{B} p(A, B)$
- $p(B) = \sum_{A} p(A, B)$

También podemos calcular la distribución condicional usando la conjunta y la marginal:

$$
p(A|B) = \frac{p(A, B)}{p(B)} = \frac{p(A, B)}{\sum_{A} p(A, B)}
$$

$$
p(B|A) = \frac{p(A, B)}{p(A)} = \frac{p(A, B)}{\sum_{B} p(A, B)}
$$

Como sabemos que la marginal se puede calcular a partir de la conjunta, solo necesitamos la conjunta para calcular la condicional.

Podemos calcular la distribución condicional inversa si solo tenemos la condicional y la marginal. Usando la regla de las probabilidades condicionales, podemos reorganizar los términos:

$$
p(B|A) = \frac{p(A, B)}{p(A)} = \frac{p(A|B)p(B)}{\sum_{B} p(A|B)p(B)}
$$

Esto implica que podemos calcular la distribución conjunta a partir de la condicional y la marginal, lo que nos lleva al Teorema de Bayes.

Para variables continuas, las sumas se convierten en integrales:

- Densidad conjunta: $ f(x, y) $
- Densidades marginales: $ f(x) $, $ f(y) $

$$
f(x) = \int f(x, y) \, dy
$$

$$
f(y) = \int f(x, y) \, dx
$$

Y la distribución condicional:

$$
f(x|y) = \frac{f(x, y)}{f(y)} = \frac{f(x, y)}{\int f(x, y) \, dx}
$$

$$
f(y|x) = \frac{f(x, y)}{f(x)} = \frac{f(x, y)}{\int f(x, y) \, dy}
$$

Además, la regla de Bayes también se expresa de forma análoga:

$$
f(x|y) = \frac{f(y|x)f(x)}{\int f(y|x)f(x) \, dx}
$$

$$
f(y|x) = \frac{f(x|y)f(y)}{\int f(x|y)f(y) \, dy}
$$

Simplemente reemplazamos la suma con una integral sobre la variable aleatoria relevante.

# MÁXIMA VEROSIMILITUD: DISTRIBUCIÓN DE BERNOULLI

La estimación de máxima verosimilitud es una técnica de estadística que consiste en ajustar un modelo a los datos para encontrar los mejores parámetros que modelen los datos lo más cerca posible de la realidad. Este concepto es fundamental en el aprendizaje de modelos en Machine Learning, como los modelos de clasificación y las redes neuronales en el aprendizaje profundo, así como en modelos de regresión lineal simple.

El ejemplo clásico de distribución de Bernoulli es el lanzamiento de una moneda. Supongamos que la probabilidad de sacar cara es de 50% y de sacar cruz es igual, lo que comúnmente se asume. Sin embargo, puede darse el caso de que la moneda no sea perfecta y estas probabilidades sean distintas.

La función de masa de probabilidad para la distribución de Bernoulli se describe como:

$$
p(x) = \theta^x(1 - \theta)^{1-x}
$$

Donde $\theta$ es la probabilidad de éxito (sacar cara, por ejemplo).