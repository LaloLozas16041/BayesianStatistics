# MOTIVACIÓN DEL ENFOQUE BAYESIANO

La Estadística Bayesiana es crucial en Estadística, Ciencia de Datos y Machine Learning. Se aplica en medicina, estimando enfermedades y en marketing digital para evaluar anuncios. Este enfoque se actualiza con datos nuevos, contrario a los métodos "online" estáticos.

Se destaca en pruebas A/B, seleccionando la opción que maximiza beneficios. Por ejemplo, Facebook optimiza qué anuncios mostrar para beneficiar al usuario y al anunciante. La Estadística Bayesiana se basa en la interpretación de la probabilidad como una medida de credibilidad.

Se abordarán los fundamentos, como la definición de probabilidad y el razonamiento dentro del marco bayesiano, preparando para métodos más avanzados de inferencia bayesiana y análisis de datos.

## Conceptos Básicos

### Distribución Conjunta, Marginales y Condicionales

Vamos a suponer que tenemos dos variables aleatorias $A$ y $B$, las distribuciones marginales serían:

- $p(A)$
- $p(B)$

La distribución conjunta la denotaremos como $p(A, B)$. La coma es una forma abreviada de decir "y".

La distribución condicional, a la que nos referimos como $p(A|B)$ o $p(B|A)$.

La barra vertical es el símbolo que usamos para representar una condición. La distribución conjunta es la más general porque es a partir de esta distribución que podemos calcular todo lo demás.

Podemos calcular la distribución marginal si tenemos la distribución conjunta, de la siguiente manera:

- $p(A) = \sum_{B} p(A, B)$
- $p(B) = \sum_{A} p(A, B)$

También podemos calcular la distribución condicional usando la conjunta y la marginal:

$$
p(A|B) = \frac{p(A, B)}{p(B)} = \frac{p(A, B)}{\sum_{A} p(A, B)}
$$

$$
p(B|A) = \frac{p(A, B)}{p(A)} = \frac{p(A, B)}{\sum_{B} p(A, B)}
$$

Como sabemos que la marginal se puede calcular a partir de la conjunta, solo necesitamos la conjunta para calcular la condicional.

Podemos calcular la distribución condicional inversa si solo tenemos la condicional y la marginal. Usando la regla de las probabilidades condicionales, podemos reorganizar los términos:

$$
p(B|A) = \frac{p(A, B)}{p(A)} = \frac{p(A|B)p(B)}{\sum_{B} p(A|B)p(B)}
$$

Esto implica que podemos calcular la distribución conjunta a partir de la condicional y la marginal, lo que nos lleva al Teorema de Bayes.

Para variables continuas, las sumas se convierten en integrales:

- Densidad conjunta: $f(x, y)$
- Densidades marginales: $f(x) $, $ f(y)$

$$
f(x) = \int f(x, y) \, dy
$$

$$
f(y) = \int f(x, y) \, dx
$$

Y la distribución condicional:

$$
f(x|y) = \frac{f(x, y)}{f(y)} = \frac{f(x, y)}{\int f(x, y) \, dx}
$$

$$
f(y|x) = \frac{f(x, y)}{f(x)} = \frac{f(x, y)}{\int f(x, y) \, dy}
$$

Además, la regla de Bayes también se expresa de forma análoga:

$$
f(x|y) = \frac{f(y|x)f(x)}{\int f(y|x)f(x) \, dx}
$$

$$
f(y|x) = \frac{f(x|y)f(y)}{\int f(x|y)f(y) \, dy}
$$

Simplemente reemplazamos la suma con una integral sobre la variable aleatoria relevante.

# MÁXIMA VEROSIMILITUD: DISTRIBUCIÓN DE BERNOULLI

La estimación de máxima verosimilitud es una técnica de estadística que consiste en ajustar un modelo a los datos para encontrar los mejores parámetros que modelen los datos lo más cerca posible de la realidad. Este concepto es fundamental en el aprendizaje de modelos en Machine Learning, como los modelos de clasificación y las redes neuronales en el aprendizaje profundo, así como en modelos de regresión lineal simple.

El ejemplo clásico de distribución de Bernoulli es el lanzamiento de una moneda. Supongamos que la probabilidad de sacar cara es de 50% y de sacar cruz es igual, lo que comúnmente se asume. Sin embargo, puede darse el caso de que la moneda no sea perfecta y estas probabilidades sean distintas.

La función de masa de probabilidad para la distribución de Bernoulli se describe como:

$$
p(x) = \theta^x(1 - \theta)^{1-x}
$$

Donde $\theta$ es la probabilidad de éxito (sacar cara, por ejemplo).

En este caso, \( x \) solo puede tomar valores cero o uno. Si pensamos en el ejemplo de lanzar una moneda, entonces generalmente diríamos que cero es cruz y uno es igual a cara. Theta (\( \theta \)) se llama parámetro y es el único parámetro de esta distribución. 

La función de masa de probabilidad para \( x \) igual a uno es:

$$
p(x = 1) = \theta^1(1 - \theta)^{1-1} = \theta
$$

Y para \( x \) igual a cero sería:

$$
p(x = 0) = 1 - \theta
$$

La función de verosimilitud para un conjunto de datos \( data = \{x_1, x_2, x_3, ..., x_n\} \) es:

$$
L(\theta) = p(data|\theta) = \prod_{i=1}^{n} p(x_i|\theta)
$$

Y esto se puede expresar como:

$$
L(\theta) = p(data|\theta) = \prod_{i=1}^{n} \theta^{x_i} (1 - \theta)^{1-x_i}
$$

Esto no es solo una ecuación matemática, tiene significado. Esto es la probabilidad de observar los datos que observamos, suponiendo que cada lanzamiento de la moneda fuera iid (independiente e idénticamente distribuido).

Es importante comprender bien la función de verosimilitud y entender de qué realmente es una función y de qué no lo es. En realidad, aquí la variable es \( \theta \) que es el valor que estamos tratando de resolver o de encontrar, porque es el parámetro de nuestra distribución.

Entonces, imagina que tenemos tres resultados del lanzamiento de la moneda, cara, cruz y cara:

\( x_1 = 1, x_2 = 0, x_3 = 1 \)

Luego los incluiríamos en la ecuación para la verosimilitud y obtendríamos lo que vemos aquí que es obviamente una función de theta, el parámetro:

$$
L(\theta) = \prod_{i=1}^{3} \theta^{x_i}(1 - \theta)^{1-x_i} = \theta^1(1 - \theta)^0\theta^1
$$

Bien, recordemos lo que estábamos haciendo. Hablamos del problema de la máxima verosimilitud y por qué se llama así. Queremos encontrar el valor de \( \theta \) que hace que nuestra función de verosimilitud sea máxima, es decir, que la probabilidad de los datos observados sea la mayor posible.

Si tenemos resultados de lanzamientos de una moneda, queremos maximizar la función de verosimilitud para encontrar el valor de \( \theta \).

Para ello, tomamos el logaritmo de la función de verosimilitud, que es más sencillo de maximizar debido a sus propiedades, y encontramos la derivada con respecto a \( \theta \):

Log-verosimilitud:

$$
l(\theta) = \log(L(\theta)) = \log \left( \prod_{i=1}^{n} p(x_i|\theta) \right) = \sum_{i=1}^{n} \log(p(x_i|\theta))
$$

Derivamos \( l(\theta) \):

$$
\frac{d}{d\theta} l(\theta) = \frac{1}{\theta} \sum_{i=1}^{n} x_i - \frac{1}{1 - \theta} \sum_{i=1}^{n} (1 - x_i)
$$

Igualamos a cero para encontrar \( \theta \):

$$
\frac{1}{\theta} \sum_{i=1}^{n} x_i - \frac{1}{1 - \theta} \sum_{i=1}^{n} (1 - x_i) = 0
$$

Resolviendo para \( \theta \):

$$
\theta = \frac{1}{n} \sum_{i=1}^{n} x_i
$$

Entonces, ¿tiene sentido este resultado? Claro, se alinea con lo que intuimos sobre probabilidad en experimentos aleatorios como el lanzamiento de monedas.

### Ejemplo Ratio de Clicqueo

Un área donde aplicamos conceptos similares es en el análisis de datos web, como el ratio de clicqueo (CTR). Este se asemeja a la probabilidad de obtener "cara" en nuestros experimentos aleatorios y es esencial en publicidad online y marketing digital.

El CTR se puede ver como la probabilidad de un usuario haciendo clic en un anuncio. Similarmente, la tasa de conversión se relaciona con la probabilidad de acciones deseadas, como compras o suscripciones.

