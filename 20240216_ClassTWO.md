La estadística bayesiana es una rama de la estadística en la que se utiliza el teorema de Bayes para actualizar la probabilidad de que una hipótesis sea cierta a medida que se disponga de más evidencia. Esta aproximación es especialmente útil en la inferencia estadística, donde se combina información previa (o conocimientos previos) con datos observados para hacer estimaciones o predicciones.

### Teorema de Bayes

El teorema de Bayes se puede expresar como:

\[ P(H|E) = \frac{P(E|H) \cdot P(H)}{P(E)} \]

Donde:
- \(P(H|E)\) es la probabilidad posterior de la hipótesis \(H\) dado el evento \(E\).
- \(P(E|H)\) es la probabilidad de observar el evento \(E\) dado que la hipótesis \(H\) es cierta.
- \(P(H)\) es la probabilidad previa de \(H\), antes de ver el evento \(E\).
- \(P(E)\) es la probabilidad de observar el evento \(E\) bajo todas las hipótesis posibles, conocida como probabilidad marginal de \(E\).

### Racional del Enfoque Bayesiano

El enfoque bayesiano se basa en la actualización de nuestras creencias en la presencia de nueva evidencia. La probabilidad previa \(P(H)\) representa nuestro conocimiento o creencia antes de observar los datos. A medida que obtenemos nueva información (evidencia \(E\)), actualizamos esta creencia para obtener una probabilidad posterior \(P(H|E)\).

La belleza del enfoque bayesiano radica en su flexibilidad para incorporar conocimientos previos y su capacidad para actualizar nuestras estimaciones a medida que se dispone de nueva información.

### Desarrollo con Ejemplos

**Ejemplo 1: Prueba de una enfermedad**

Imagina que estamos tratando de estimar la probabilidad de que una persona tenga una enfermedad basándonos en el resultado de una prueba médica.

- Supongamos que la probabilidad previa de tener la enfermedad (la prevalencia) es del 0.1% (\(P(H) = 0.001\)).
- La prueba tiene una tasa de verdaderos positivos (sensibilidad) del 99% (\(P(E|H) = 0.99\)) y una tasa de falsos positivos (1 - especificidad) del 1% (\(P(E|\neg H) = 0.01\)).

Queremos calcular la probabilidad de que alguien realmente tenga la enfermedad si da positivo en la prueba (\(P(H|E)\)).

Primero, necesitamos calcular \(P(E)\), la probabilidad de dar positivo. Esto se puede hacer usando la ley total de probabilidad:

\[ P(E) = P(E|H)P(H) + P(E|\neg H)P(\neg H) \]

Vamos a calcularlo:

\[ P(E) = 0.99 \cdot 0.001 + 0.01 \cdot 0.999 = 0.01098 \]

Ahora, usando el teorema de Bayes:

\[ P(H|E) = \frac{0.99 \cdot 0.001}{0.01098} \approx 0.09016 \]

Esto significa que, incluso con un resultado positivo en una prueba altamente sensible, la probabilidad de que la persona realmente tenga la enfermedad es solo del aproximadamente 9.02%, debido a la baja prevalencia de la enfermedad.

**Ejemplo 2: Lanzamiento de moneda**

Supongamos que tienes una moneda y quieres estimar la probabilidad de que caiga cara (\(H\)). Antes de lanzar la moneda, tu creencia previa es que hay un 50% de probabilidad de que caiga cara (\(P(H) = 0.5\)).

Si lanzas la moneda 10 veces y observas 7 caras (\(E\)), puedes actualizar tu creencia sobre la probabilidad de que la moneda caiga cara usando el teorema de Bayes.

La actualización precisa en este caso implicaría calcular la verosimilitud de observar los datos dado el modelo, lo cual requiere un modelo estadístico más complejo para la distribución de los datos (como la distribución binomial en este caso). Sin embargo, el proceso conceptual sigue siendo el mismo: ajustas tu creencia previa basándote en la evidencia observada para obtener una nueva probabilidad posterior.

Estos ejemp

los ilustran cómo el teorema de Bayes permite incorporar evidencia nueva a nuestras creencias previas de una manera coherente y matemáticamente rigurosa, proporcionando una herramienta poderosa para la inferencia estadística.